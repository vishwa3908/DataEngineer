{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('questions').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-------------+----------+----+\n",
      "|             PLAYERS|NATIONALITY|         TYPE|PRICE PAID|TEAM|\n",
      "+--------------------+-----------+-------------+----------+----+\n",
      "|Avanish Rao Aravelly|     Indian|Wicket-Keeper|   2000000| CSK|\n",
      "|   Mustafizur Rahman|   Overseas|       Bowler|  20000000| CSK|\n",
      "|      Daryl Mitchell|   Overseas|  All-Rounder| 140000000| CSK|\n",
      "|        Sameer Rizvi|     Indian|       Batter|  84000000| CSK|\n",
      "|     Rachin Ravindra|   Overseas|  All-Rounder|  18000000| CSK|\n",
      "+--------------------+-----------+-------------+----------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+-----------+-----------+----------+\n",
      "|         PLAYER|NATIONALITY|       TYPE|BASE PRICE|\n",
      "+---------------+-----------+-----------+----------+\n",
      "|  Priyansh Arya|     Indian|     Batter|   2000000|\n",
      "|Rohan Kunnummal|     Indian|     Batter|   2000000|\n",
      "|    Manan Vohra|     Indian|     Batter|   2000000|\n",
      "| Raj Angad Bawa|     Indian|All-Rounder|   2000000|\n",
      "|  Sarfaraz Khan|     Indian|All-Rounder|   2000000|\n",
      "+---------------+-----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------+-----------+-------------+\n",
      "|                TEAM|   NATIONALITY|       TYPE|   PRICE PAID|\n",
      "+--------------------+--------------+-----------+-------------+\n",
      "|Kolkata Knight Ri...|Mitchell Starc|     Bowler| 24,75,00,000|\n",
      "| Sunrisers Hyderabad|   Pat Cummins|All-Rounder| 20,50,00,000|\n",
      "| Chennai Super Kings|Daryl Mitchell|All-Rounder| 14,00,00,000|\n",
      "|        Punjab Kings| Harshal Patel|All-Rounder| 11,75,00,000|\n",
      "|Royal Challengers...|Alzarri Joseph|     Bowler| 11,50,00,000|\n",
      "+--------------------+--------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_players_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('./csv/IPL_PLAYERS.csv')\n",
    "unsold_payers_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('./csv/UNSOLD_PLAYERS.csv')\n",
    "top_buys_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('./csv/TOP_BUYS.csv')\n",
    "all_players_df.show(5)\n",
    "unsold_payers_df.show(5)\n",
    "top_buys_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Create a dataset from this having origin and destination only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+\n",
      "|cust_id|   Source|Destination|\n",
      "+-------+---------+-----------+\n",
      "|      1|    Delhi|  Mangalore|\n",
      "|      2|   Mumbai|  Gorakhpur|\n",
      "|      3|  Chennai|        Goa|\n",
      "|      4|  Kolkata|  Ahmedabad|\n",
      "|      5|Hyderabad|Bhubaneswar|\n",
      "|      6|Bengaluru|     Jaipur|\n",
      "|      7|  Lucknow|    Kolkata|\n",
      "|      8|   Indore|     Bhopal|\n",
      "+-------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df = spark.read.format('csv').option('header',True).option('inferSchema',True).load('./csv/flight_data.csv')\n",
    "\n",
    "# Method 1\n",
    "# flight_df.show()\n",
    "flight_df_1 = flight_df.withColumn('row_num',row_number().over(Window.partitionBy(col('cust_id')).orderBy(col('cust_id'))))\n",
    "# flight_df.show()\n",
    "flight_df_1 = flight_df_1.groupBy(col('cust_id')).agg(first(col('origin')).alias('Source'),last(col('destination')).alias('Destination'))\n",
    "flight_df_1.show()\n",
    "\n",
    "# Method 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Questions\n",
    "\n",
    "1. Difference between client and cluster mode\n",
    "\n",
    "2. what is partition skew, reasons for it. How to solve partition skew issues?\n",
    "\n",
    "3. what is a broadcast join in apache spark\n",
    "\n",
    "4. what is the difference between partition and bucketing\n",
    "\n",
    "5. What are different types of joins in Spark\n",
    "\n",
    "6. why count when used with group by is a transformation else its an action.\n",
    "\n",
    "7. If your spark job is running slow how would you approach to debug it.\n",
    "\n",
    "8. Difference between managed table & external table. When do you go about creating exernal tables.\n",
    "\n",
    "9. Why we are not using mapreduce these days. what are similarities between spark and mapReduce.\n",
    "\n",
    "10. How do you handle your pyspark code deployment, Explain about the CICD process.\n",
    "\n",
    "11. Have you used caching in your project, when & where do you consider using it.\n",
    "\n",
    "12. how to estimate the amount of resources for your spark job.\n",
    "\n",
    "13. difference between narrow and wide transformation\n",
    "\n",
    "14. difference between dataframe and dataset\n",
    "\n",
    "15. If some job failed with out of memory error in production, what will be you approach to debug that\n",
    "\n",
    "16. what is DAG & how that helps.\n",
    "\n",
    "17. which version control do you use\n",
    "\n",
    "18. how do you test your spark code\n",
    "\n",
    "19. what is shuffling, why we should think about minimizing it.\n",
    "\n",
    "20. if 199/200 partitions are getting executed but after 1 hour you are getting error. What things you will do?\n",
    "\n",
    "21. How spark achieves fault tolerance?\n",
    "\n",
    "22. What is lazy evaluation in spark?\n",
    "\n",
    "23. What is the difference between lineage and dag?\n",
    "\n",
    "24. What is difference between Persist() and cache()\n",
    "\n",
    "25. How is spark sql different from HQL?\n",
    "\n",
    "26. How to join two different tables using Dataframes?\n",
    "\n",
    "27. How to tune spark executor and executor memory?\n",
    "\n",
    "28. Explain about dynamic allocation in spark?\n",
    "\n",
    "29. Why dataset is preferred compared to dataframe?\n",
    "\n",
    "30. why spark over map-reduce?\n",
    "\n",
    "31. What are the different mode in spark?\n",
    "\n",
    "32. What is Map and FlatMap operation in spark?\n",
    "\n",
    "33. What are the challenges you face in spark?\n",
    "\n",
    "34. What is rdd lineage?\n",
    "\n",
    "35. Major issues faced in spark development?\n",
    "\n",
    "36. Optimizations technique in spark?\n",
    "\n",
    "37. What is difference between reduceByKey and GroupByKey\n",
    "\n",
    "38. How will you join two bigger table in spark?\n",
    "\n",
    "39. What is difference between repartition and Coalesce?\n",
    "\n",
    "40. What is checkpointing in spark?\n",
    "\n",
    "41. What's the difference between an RDD, a DataFrame, and a DataSet?\n",
    "\n",
    "42. How can you create a DataFrame a) using existing RDD, and b) from a CSV file?\n",
    "\n",
    "43. Explain the use of StructType and StructField classes in PySpark with examples.\n",
    "\n",
    "44. What are the different ways to handle row duplication in a PySpark DataFrame?\n",
    "\n",
    "45. Explain PySpark UDF with the help of an example.\n",
    "\n",
    "46. Discuss the map() transformation in PySpark \n",
    "DataFrame with the help of an example.\n",
    "\n",
    "47. What do you mean by ‘joins’ in PySpark DataFrame? What are the different types of joins?\n",
    "\n",
    "48. What is PySpark ArrayType? Explain with an example.\n",
    "\n",
    "49. What do you understand by PySpark Partition?\n",
    "\n",
    "50. What is meant by PySpark MapType? How can you create a MapType using StructType?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Differenece between client and cluster mode ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "In client mode\n",
    "\n",
    "lets say user comes submit the job using spark submit comes to edge node (gateway to the clusters) and executed spark submit job in client mode , driver program will launch to the edge node itself which is a part of the cluster and this driver program will spawn different executors on different nodes of the cluster. So, driver program will be occupying resources (memory,cpu) of the edge node. \n",
    "lets say another user comes and submit another job . another driver program will launch and occupy resources of the edge nodes.\n",
    "\n",
    "Client Mode:\n",
    "\n",
    "* In client mode, the driver program runs on the machine where you submit the Spark application. \n",
    "* It communicates with the cluster manager to request resources and execute tasks on the worker nodes.\n",
    "* The SparkContext is initialized on the client machine, and the driver program coordinates the execution of tasks on the cluster.\n",
    "* This mode is often used for development and debugging, allowing you to interactively examine results and debug your application more easily.\n",
    "\n",
    "Cluster Mode:\n",
    "\n",
    "* In cluster mode, the driver program runs on one of the nodes in the Spark cluster. \n",
    "* The SparkContext is initialized on that node, and the driver program is responsible for managing the execution of the application.\n",
    "* The driver program typically runs on the master node, and the Spark workers run on the worker nodes.\n",
    "* This mode is suitable for large-scale production deployments where the Spark application is submitted to a cluster manager (like Apache YARN, Apache Mesos, or Spark's standalone cluster manager).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark submit -> submit the spark job -> option to specify deploy mode(client or cluster) -> \n",
    "\n",
    "after job submit -> driver program spawned (drives complete spark jobs)  -> executor program spawned -> spwaned on different nodes (data processing is done here and controlled by driver program)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. what is partition skew, reasons for it. How to solve partition skew issues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "It occurs when the distribution of data across the partitions in uneven, leading to some partitions having much larger data size then the other partitions. Example: a single partition handles maximum amount of data which lead to skewness. \n",
    "\n",
    "reason  -  mainly happens after wide transformation\n",
    "\n",
    "solve - \n",
    "1. salting - if a key is occuring multiple times add some random number to  it, one key splitting into multiple keys. (complex process)\n",
    "2. AQE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. what is a broadcast join in apache spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution -\n",
    "sometimes also called map side join\n",
    "whenever we have one small table and one  large table you can use broadcast join\n",
    "small table can be broadcasted across all the executors \n",
    "adv -  no shuffling of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. what is the difference between partition and bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "partitioning - both are strategis to structure data so that you can read some data and skip most. \n",
    "\n",
    "when we have a column with less number of distinct column then we can use partitioning\n",
    "folders created on logic\n",
    "\n",
    "bucketing - files created based on hash function. also helps in join operation when joining two large tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. What are the different types of joins in Spark ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "1. broadcast hash join - one small table and one large table - no shuffling involved\n",
    "2. shuffle hash join - slight optimization of later - one medium and one large table - from medium table hash is created so no sorting is involved.\n",
    "3. shuffle sort merge join - 2 large tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. why count when used with group by is a transformation else its an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "df.count() -  action\n",
    "\n",
    "df.groupBy().count() -  transformation further calculation is possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7. If your spark job is running slow how would you approach to debug it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "1. check spark UI for your slow tasks , enable AQE for handling partition skew\n",
    "2. optimize join strategies , consider broadcast join for small datasets\n",
    "3. ensure sufficient resources are allocated for your job\n",
    "4. verify number of dataframe partitions / change number of shuffle partition if required\n",
    "5. mitigate garbage collection delays by giving some off heap memory\n",
    "6. monitor disk spills and allocate more memory per cpu core if needed\n",
    "7. opt for hash aggregation over sort aggregate when aggregating\n",
    "8. implement caching\n",
    "9. choose right file format and fast compressing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8. Difference between managed and external tables. when do you go about creating external tables ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "table -  data + metadata\n",
    "\n",
    "managed -  data and metadata managed by spark\n",
    "external -  data is external but metadata is managed\n",
    "\n",
    "when you drop managed table both data and metadata get dropped. \n",
    "when you drop external table only metadata gets dropped,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9. Why we are not using mapreduce these days. what are similarities between spark and mapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "limitation of map reduce\n",
    "1. slow\n",
    "2. heavy use of disk , writes on disk\n",
    "3. can be only written in java , have to write a lot of code\n",
    "\n",
    "similarities\n",
    "1. both are distributing engine\n",
    "\n",
    "spark is faster, generic , in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13. What are the differences between narrow and wide transformation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "Narrow Transformation :-     Narrow transformations are the result of map and filter functions and these compute data that live on a single partition meaning there will not be any data movement between partitions to execute narrow transformations.Functions such as `map()`, `mapPartition()`, `flatMap()`, `filter()`, `union()` are some examples of narrow transformation\n",
    "\n",
    "\n",
    "\n",
    "Wide Transformation :-  Wide transformations are the result of groupByKey and reduceByKey functions and these compute data that live on many partitions meaning there will be data movements between partitions to execute wide transformations. Since these shuffles the data, they also called shuffle transformations.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Context serves as an entry point to spark, manages cluster resources, and spark configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Architecture ->\n",
    "\n",
    "it has master-slave architecture\n",
    " consists of driver and executors which run as master and worker node respectively.\n",
    " we have spark context \n",
    " cluster manager used for allocation of resources \n",
    "\n",
    "\n",
    "\n",
    "when the spark program is submitted the driver program in the master node initializes the spark context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you need to join two large dataset but the join operation causing out of memory error. what strategy would you use to optimize this join\n",
    "\n",
    "1. Broadcast Join\n",
    "If one of the datasets is small enough to fit into memory, you can use a broadcast join. This avoids shuffling the large dataset and improves performance.\n",
    "2. Increase Shuffle Partitions\n",
    "Increasing the number of shuffle partitions can help distribute the data more evenly across the cluster, reducing the memory load on individual nodes\n",
    "3. Repartition Before Join\n",
    "Repartitioning the datasets before joining can ensure better data distribution and reduce skew\n",
    "4. Use Bucketing\n",
    "If you perform the same join multiple times, consider using bucketing. This can reduce the shuffle overhead.\n",
    "5. Filter Early\n",
    "Apply filters to reduce the amount of data before performing the join.\n",
    "6. Adjust Spark Configuration\n",
    "Tuning Spark configuration parameters can help manage memory usage better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
